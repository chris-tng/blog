<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Tips on converting pytorch models to flax | Chris Nguyen’s notes</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Tips on converting pytorch models to flax" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="You probably hear about jax. As all the cool kids in town use it, I decided to give it a try a while ago. But not until recently, during an experiment with TPU in Pytorch, I ran into a fatal error due to limited support of pytorch for XLA. With my limited knowledge of jax/flax, I decided to port huggingface’s ELECTRA model to flax. This results in my first :hugs: pull request. This blog contains a few tips for converting pytorch to flax model." />
<meta property="og:description" content="You probably hear about jax. As all the cool kids in town use it, I decided to give it a try a while ago. But not until recently, during an experiment with TPU in Pytorch, I ran into a fatal error due to limited support of pytorch for XLA. With my limited knowledge of jax/flax, I decided to port huggingface’s ELECTRA model to flax. This results in my first :hugs: pull request. This blog contains a few tips for converting pytorch to flax model." />
<link rel="canonical" href="https://chris-tng.github.io/blog/transformer/nlp/flax/pytorch/jax/2020/12/16/tips-flax.html" />
<meta property="og:url" content="https://chris-tng.github.io/blog/transformer/nlp/flax/pytorch/jax/2020/12/16/tips-flax.html" />
<meta property="og:site_name" content="Chris Nguyen’s notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-16T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"You probably hear about jax. As all the cool kids in town use it, I decided to give it a try a while ago. But not until recently, during an experiment with TPU in Pytorch, I ran into a fatal error due to limited support of pytorch for XLA. With my limited knowledge of jax/flax, I decided to port huggingface’s ELECTRA model to flax. This results in my first :hugs: pull request. This blog contains a few tips for converting pytorch to flax model.","headline":"Tips on converting pytorch models to flax","dateModified":"2020-12-16T00:00:00-06:00","datePublished":"2020-12-16T00:00:00-06:00","@type":"BlogPosting","url":"https://chris-tng.github.io/blog/transformer/nlp/flax/pytorch/jax/2020/12/16/tips-flax.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://chris-tng.github.io/blog/transformer/nlp/flax/pytorch/jax/2020/12/16/tips-flax.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://chris-tng.github.io/blog/feed.xml" title="Chris Nguyen's notes" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Tips on converting pytorch models to flax | Chris Nguyen’s notes</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Tips on converting pytorch models to flax" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="You probably hear about jax. As all the cool kids in town use it, I decided to give it a try a while ago. But not until recently, during an experiment with TPU in Pytorch, I ran into a fatal error due to limited support of pytorch for XLA. With my limited knowledge of jax/flax, I decided to port huggingface’s ELECTRA model to flax. This results in my first :hugs: pull request. This blog contains a few tips for converting pytorch to flax model." />
<meta property="og:description" content="You probably hear about jax. As all the cool kids in town use it, I decided to give it a try a while ago. But not until recently, during an experiment with TPU in Pytorch, I ran into a fatal error due to limited support of pytorch for XLA. With my limited knowledge of jax/flax, I decided to port huggingface’s ELECTRA model to flax. This results in my first :hugs: pull request. This blog contains a few tips for converting pytorch to flax model." />
<link rel="canonical" href="https://chris-tng.github.io/blog/transformer/nlp/flax/pytorch/jax/2020/12/16/tips-flax.html" />
<meta property="og:url" content="https://chris-tng.github.io/blog/transformer/nlp/flax/pytorch/jax/2020/12/16/tips-flax.html" />
<meta property="og:site_name" content="Chris Nguyen’s notes" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-12-16T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"You probably hear about jax. As all the cool kids in town use it, I decided to give it a try a while ago. But not until recently, during an experiment with TPU in Pytorch, I ran into a fatal error due to limited support of pytorch for XLA. With my limited knowledge of jax/flax, I decided to port huggingface’s ELECTRA model to flax. This results in my first :hugs: pull request. This blog contains a few tips for converting pytorch to flax model.","headline":"Tips on converting pytorch models to flax","dateModified":"2020-12-16T00:00:00-06:00","datePublished":"2020-12-16T00:00:00-06:00","@type":"BlogPosting","url":"https://chris-tng.github.io/blog/transformer/nlp/flax/pytorch/jax/2020/12/16/tips-flax.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://chris-tng.github.io/blog/transformer/nlp/flax/pytorch/jax/2020/12/16/tips-flax.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://chris-tng.github.io/blog/feed.xml" title="Chris Nguyen's notes" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Chris Nguyen&#39;s notes</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Tips on converting pytorch models to flax</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-12-16T00:00:00-06:00" itemprop="datePublished">
        Dec 16, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#transformer">transformer</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#nlp">nlp</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#flax">flax</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#pytorch">pytorch</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#jax">jax</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#why-flax">Why flax</a></li>
<li class="toc-entry toc-h2"><a href="#loading-pytorch-checkpoint">Loading pytorch checkpoint</a></li>
<li class="toc-entry toc-h2"><a href="#converting-to-nested-dict">Converting to nested dict</a></li>
<li class="toc-entry toc-h2"><a href="#divide-and-conquer-to-align-model-weights">Divide and Conquer to align model weights</a></li>
<li class="toc-entry toc-h2"><a href="#prefer-setup-instead-of-nncompact">Prefer setup instead of nn.compact</a></li>
<li class="toc-entry toc-h2"><a href="#common-problems-to-keep-in-mind">Common problems to keep in mind</a></li>
<li class="toc-entry toc-h2"><a href="#conclusion">Conclusion</a></li>
</ul><p>You probably hear about <a href="https://github.com/google/jax">jax</a>. As all the cool kids in town use it, I decided to give it a try a while ago. But not until recently, during an experiment with TPU in Pytorch, I ran into a fatal error due to limited support of pytorch for XLA. With my limited knowledge of <code class="highlighter-rouge">jax/flax</code>, I decided to port huggingface’s <a href="https://ai.googleblog.com/2020/03/more-efficient-nlp-model-pre-training.html">ELECTRA</a> model to <a href="https://github.com/google/flax">flax</a>. This results in my first :hugs:  <a href="https://github.com/huggingface/transformers/pull/9172">pull request</a>. This blog contains a few tips for converting pytorch to flax model.</p>

<h2 id="why-flax">
<a class="anchor" href="#why-flax" aria-hidden="true"><span class="octicon octicon-link"></span></a>Why flax</h2>

<p>You may ask why I use <code class="highlighter-rouge">flax</code>. To be honest, I don’t know. I’m naive user just like many of you guys. Part of the reason is <code class="highlighter-rouge">flax</code> seems to be getting some traction nowadays. Especially, huggingface :hugs: starts to roll out some support for flax (with <a href="https://huggingface.co/transformers/model_doc/bert.html#flaxbertmodel">FlaxBertModel</a>).</p>

<p>Personally, as a pytorch user (with a painful background in tensorflow), I find <code class="highlighter-rouge">flax</code> has a steep learning curve. I don’t really like the abstractions <code class="highlighter-rouge">flax</code> introduce, it feels like magic! However, among many competing frameworks in <code class="highlighter-rouge">jax</code> space (<a href="https://github.com/deepmind/dm-haiku">haiku</a>, <a href="https://github.com/google/trax">trax</a>, <a href="https://github.com/google/objax">objax</a>), I had to pick one, and that’s flax. I may deeply regret my choice in the future, who knows. But c’est la vie!</p>

<p>Okay, let’s start.</p>

<h2 id="loading-pytorch-checkpoint">
<a class="anchor" href="#loading-pytorch-checkpoint" aria-hidden="true"><span class="octicon octicon-link"></span></a>Loading pytorch checkpoint</h2>

<p>After you download and cache model</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">generator</span> <span class="o">=</span> <span class="n">ElectraForMaskedLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
</code></pre></div></div>

<p>The model binary and its JSON config are cached under <code class="highlighter-rouge">~/.cache/huggingface/transformers/</code> with long filenames (corresponding to Amazon S3 hashes).</p>

<p>You can load the binary to a python dict</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">model_file</span> <span class="o">=</span> <span class="s">"~/.cache/huggingface/transformers/blablabla"</span>
<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">model_file</span><span class="p">,</span> <span class="s">"rb"</span><span class="p">)</span> <span class="k">as</span> <span class="n">state_f</span><span class="p">:</span>
    <span class="n">pt_state</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">state_f</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cpu"</span><span class="p">))</span>
    <span class="n">pt_state</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">pt_state</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">pt_state</code> is a flat python dict, first few keys looks like</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">'electra.embeddings.word_embeddings.weight'</span><span class="p">,</span> <span class="s">'electra.embeddings.position_embeddings.weight'</span><span class="p">,</span> <span class="s">'electra.embeddings.token_type_embeddings.weight'</span>
</code></pre></div></div>

<p><code class="highlighter-rouge">jax/flax</code> uses nested dict to manage model parameters so a conversion is needed</p>

<h2 id="converting-to-nested-dict">
<a class="anchor" href="#converting-to-nested-dict" aria-hidden="true"><span class="octicon octicon-link"></span></a>Converting to nested dict</h2>

<p>In a beautiful and happy world, we could do</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">FlaxElectraForMaskedLM</span>
<span class="kn">from</span> <span class="nn">flax.traverse_util</span> <span class="kn">import</span> <span class="n">flatten_dict</span><span class="p">,</span> <span class="n">unflatten_dict</span>

<span class="n">fx_state</span> <span class="o">=</span> <span class="n">FlaxElectraForMaskedLM</span><span class="o">.</span><span class="n">convert_from_pytorch</span><span class="p">(</span><span class="n">pt_state</span><span class="p">,</span> <span class="n">config</span><span class="p">)</span>
<span class="n">fx_state</span> <span class="o">=</span> <span class="n">unflatten_dict</span><span class="p">({</span><span class="nb">tuple</span><span class="p">(</span><span class="n">k</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">"."</span><span class="p">)):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">fx_state</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span>
</code></pre></div></div>

<p>Unfortunately, we don’t have <code class="highlighter-rouge">FlaxElectraForMaskedLM</code> yet, haha! So we use a similar model to load, for example <code class="highlighter-rouge">FlaxBertPreTrainedModel</code>. The key is to override <code class="highlighter-rouge">convert_from_pytorch</code> (<a href="https://github.com/huggingface/transformers/blob/640e6fe19062bb722f06dc3303ca2b6104de367d/src/transformers/models/bert/modeling_flax_bert.py">link</a>) so that our pytorch weights are loaded correctly in <code class="highlighter-rouge">flax</code>.</p>

<h2 id="divide-and-conquer-to-align-model-weights">
<a class="anchor" href="#divide-and-conquer-to-align-model-weights" aria-hidden="true"><span class="octicon octicon-link"></span></a>Divide and Conquer to align model weights</h2>

<p>The whole effort lies in this part where I basically have to check every layer correctly loaded in <code class="highlighter-rouge">flax</code> and check the forward pass done correctly.</p>

<p>The trick to work <code class="highlighter-rouge">flax</code> is to use scope to bound flax module. Basically, flax module works in two modes: bound and unbound. In bound mode, it keeps a reference to a scope so it has access to its parameters, and we can examine them. In unbound mode, it’s no difference than function, parameters are fed from the external, the module stores nothing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">flax.core.scope</span> <span class="kn">import</span> <span class="n">Scope</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">random</span>

<span class="n">rngkey</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

<span class="c1"># testing on embeddings
</span><span class="n">scope</span> <span class="o">=</span> <span class="n">Scope</span><span class="p">({</span><span class="s">"params"</span><span class="p">:</span> <span class="n">fx_state</span><span class="p">[</span><span class="s">"embeddings"</span><span class="p">]},</span> <span class="p">{</span><span class="s">"params"</span><span class="p">:</span> <span class="n">rngkey</span><span class="p">},</span> <span class="n">mutable</span><span class="o">=</span><span class="p">[</span><span class="s">"params"</span><span class="p">])</span>

<span class="n">layer</span> <span class="o">=</span> <span class="n">FlaxBertEmbeddings</span><span class="p">(</span>
    <span class="n">vocab_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">,</span>
    <span class="n">type_vocab_size</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">type_vocab_size</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">max_position_embeddings</span><span class="p">,</span>
    <span class="n">parent</span><span class="o">=</span><span class="n">scope</span>    
<span class="p">)</span>

<span class="c1"># checking param of layer_norm
</span><span class="n">layer</span><span class="o">.</span><span class="n">children</span><span class="p">[</span><span class="s">"layer_norm"</span><span class="p">]</span>

<span class="c1"># forward pass
</span><span class="n">x_embed</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># check if it's close to output of pytorch model
</span><span class="n">jnp</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">x_embed</span><span class="p">,</span> <span class="n">x_embed_pt</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span>
</code></pre></div></div>

<p>If something went wrong in this step, you will need to make changes to the above <code class="highlighter-rouge">convert_to_pytorch</code>.</p>

<h2 id="prefer-setup-instead-of-nncompact">
<a class="anchor" href="#prefer-setup-instead-of-nncompact" aria-hidden="true"><span class="octicon octicon-link"></span></a>Prefer <code class="highlighter-rouge">setup</code> instead of <code class="highlighter-rouge">nn.compact</code>
</h2>

<p>One tip that helps me to debug sub-modules is to use <code class="highlighter-rouge">setup</code> instead of <code class="highlighter-rouge">nn.compact</code>.</p>

<p>Basically, <code class="highlighter-rouge">nn.compact</code> allows us to be lazy. It’s a decorator for the forward pass so that we can declare inlined (and lazy) sub-modules. They will be lazily initialized during the forward pass (not sure why I used so many “lazy” words, I bet it has some correlation with when I wrote this: on a Friday afternoon).</p>

<p><code class="highlighter-rouge">setup</code> initializes sub-modules the moment we create the module (as of now, this hasn’t happened yet, but hopefully <a href="https://github.com/google/flax/issues/686">soon</a>). We can examine sub-modules via attribute access like <code class="highlighter-rouge">dummy.dense</code></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">flax</span> <span class="kn">import</span> <span class="n">linen</span> <span class="k">as</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">Dummy</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span>
        
    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
<span class="n">dummy</span> <span class="o">=</span> <span class="n">Dummy</span><span class="p">(</span><span class="n">hidden_size</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">parent</span><span class="o">=</span><span class="n">Scope</span><span class="p">({},</span> <span class="p">{</span><span class="s">"params"</span><span class="p">:</span> <span class="n">rngkey</span><span class="p">},</span> <span class="n">mutable</span><span class="o">=</span><span class="p">[</span><span class="s">"params"</span><span class="p">]))</span>
<span class="n">dummy</span><span class="o">.</span><span class="n">dense</span>
</code></pre></div></div>

<h2 id="common-problems-to-keep-in-mind">
<a class="anchor" href="#common-problems-to-keep-in-mind" aria-hidden="true"><span class="octicon octicon-link"></span></a>Common problems to keep in mind</h2>

<ul>
  <li>
<code class="highlighter-rouge">flax</code> uses kernel instead of weight for parameter</li>
  <li>sometimes, you have to transpose the weight</li>
  <li>sometimes, you have to add missing sub-module</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">FlaxElectraGeneratorPredictions</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="n">embedding_size</span><span class="p">:</span> <span class="nb">int</span>
    <span class="n">hidden_act</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s">"gelu"</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">float32</span>

    <span class="k">def</span> <span class="nf">setup</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embedding_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span> <span class="o">=</span> <span class="n">FlaxElectraLayerNorm</span><span class="p">(</span><span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_states</span><span class="p">):</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="n">ACT2FN</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_act</span><span class="p">](</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="n">hidden_states</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_norm</span><span class="p">(</span><span class="n">hidden_states</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">hidden_states</span>
</code></pre></div></div>

<p>Check out my <a href="https://github.com/huggingface/transformers/pull/9172">code</a> if you want to know more.</p>

<h2 id="conclusion">
<a class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h2>

<p>The process of porting a model to pytorch is time-consuming. I hope this post can alleviate some pains in the process. I decided to give <code class="highlighter-rouge">flax</code> a serious try. I may post something about <code class="highlighter-rouge">flax</code> or <code class="highlighter-rouge">jax</code> in the future.</p>

<p>I appreciate any of your feedbacks or questions, feel free to reach out.</p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="chris-tng/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/transformer/nlp/flax/pytorch/jax/2020/12/16/tips-flax.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/chris-tng" title="chris-tng"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
