<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>First Look at Gradient Checkpointing in Pytorch | Chris Nguyen’s ML blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="First Look at Gradient Checkpointing in Pytorch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="If you are one of those like me, who couldn’t afford to rent a super duper 8-GPU Titan RTX or don’t have access to such compute, but still want to experiment with latest NLP models. You might be interested in gradient checkpoint. In this post, I’ll explore gradient checkpointing in Pytorch." />
<meta property="og:description" content="If you are one of those like me, who couldn’t afford to rent a super duper 8-GPU Titan RTX or don’t have access to such compute, but still want to experiment with latest NLP models. You might be interested in gradient checkpoint. In this post, I’ll explore gradient checkpointing in Pytorch." />
<link rel="canonical" href="https://chris-tng.github.io/blog/pytorch/2020/08/16/gradient-checkpointing-pytorch.html" />
<meta property="og:url" content="https://chris-tng.github.io/blog/pytorch/2020/08/16/gradient-checkpointing-pytorch.html" />
<meta property="og:site_name" content="Chris Nguyen’s ML blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-16T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2020-08-16T00:00:00-05:00","headline":"First Look at Gradient Checkpointing in Pytorch","description":"If you are one of those like me, who couldn’t afford to rent a super duper 8-GPU Titan RTX or don’t have access to such compute, but still want to experiment with latest NLP models. You might be interested in gradient checkpoint. In this post, I’ll explore gradient checkpointing in Pytorch.","mainEntityOfPage":{"@type":"WebPage","@id":"https://chris-tng.github.io/blog/pytorch/2020/08/16/gradient-checkpointing-pytorch.html"},"@type":"BlogPosting","url":"https://chris-tng.github.io/blog/pytorch/2020/08/16/gradient-checkpointing-pytorch.html","dateModified":"2020-08-16T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://chris-tng.github.io/blog/feed.xml" title="Chris Nguyen's ML blog" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>First Look at Gradient Checkpointing in Pytorch | Chris Nguyen’s ML blog</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="First Look at Gradient Checkpointing in Pytorch" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="If you are one of those like me, who couldn’t afford to rent a super duper 8-GPU Titan RTX or don’t have access to such compute, but still want to experiment with latest NLP models. You might be interested in gradient checkpoint. In this post, I’ll explore gradient checkpointing in Pytorch." />
<meta property="og:description" content="If you are one of those like me, who couldn’t afford to rent a super duper 8-GPU Titan RTX or don’t have access to such compute, but still want to experiment with latest NLP models. You might be interested in gradient checkpoint. In this post, I’ll explore gradient checkpointing in Pytorch." />
<link rel="canonical" href="https://chris-tng.github.io/blog/pytorch/2020/08/16/gradient-checkpointing-pytorch.html" />
<meta property="og:url" content="https://chris-tng.github.io/blog/pytorch/2020/08/16/gradient-checkpointing-pytorch.html" />
<meta property="og:site_name" content="Chris Nguyen’s ML blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-16T00:00:00-05:00" />
<script type="application/ld+json">
{"datePublished":"2020-08-16T00:00:00-05:00","headline":"First Look at Gradient Checkpointing in Pytorch","description":"If you are one of those like me, who couldn’t afford to rent a super duper 8-GPU Titan RTX or don’t have access to such compute, but still want to experiment with latest NLP models. You might be interested in gradient checkpoint. In this post, I’ll explore gradient checkpointing in Pytorch.","mainEntityOfPage":{"@type":"WebPage","@id":"https://chris-tng.github.io/blog/pytorch/2020/08/16/gradient-checkpointing-pytorch.html"},"@type":"BlogPosting","url":"https://chris-tng.github.io/blog/pytorch/2020/08/16/gradient-checkpointing-pytorch.html","dateModified":"2020-08-16T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://chris-tng.github.io/blog/feed.xml" title="Chris Nguyen's ML blog" />
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" async src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Chris Nguyen&#39;s ML blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">First Look at Gradient Checkpointing in Pytorch</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-08-16T00:00:00-05:00" itemprop="datePublished">
        Aug 16, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#pytorch">pytorch</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h3"><a href="#forward-pass">Forward pass</a></li>
<li class="toc-entry toc-h3"><a href="#backward-pass">Backward pass</a></li>
<li class="toc-entry toc-h3"><a href="#putting-it-together">Putting it together</a></li>
<li class="toc-entry toc-h3"><a href="#references">References</a></li>
</ul><p>If you are one of those like me, who couldn’t afford to rent a super duper 8-GPU Titan RTX or don’t have access to such compute, but still want to experiment with latest NLP models. You might be interested in <code class="highlighter-rouge">gradient checkpoint</code>. In this post, I’ll explore gradient checkpointing in Pytorch.</p>

<p>In brief, <code class="highlighter-rouge">gradient checkpointing</code> is a trick to save memory by recomputing the intermediate activations during backward. Think of it like “lazy” backward. Layer activations are not saved for backpropagation but recomputed when necessary.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch.utils.checkpoint</span> <span class="k">as</span> <span class="n">cp</span>

<span class="c1"># Original:
</span><span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">my_block</span><span class="p">(</span><span class="n">inp1</span><span class="p">,</span> <span class="n">inp2</span><span class="p">,</span> <span class="n">inp3</span><span class="p">)</span>

<span class="c1"># With checkpointing:
</span><span class="n">out</span> <span class="o">=</span> <span class="n">cp</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">my_block</span><span class="p">,</span> <span class="n">inp1</span><span class="p">,</span> <span class="n">inp2</span><span class="p">,</span> <span class="n">inp3</span><span class="p">)</span>
</code></pre></div></div>

<p>That looks surprisingly simple. Wondering what magic lies underneath? Let’s dive in.</p>

<h3 id="forward-pass">
<a class="anchor" href="#forward-pass" aria-hidden="true"><span class="octicon octicon-link"></span></a>Forward pass</h3>

<p>Imagine the following forward pass where input x goes through layers one by one, resulting in intermediate activations h1, h2. Normally, h1 and h2 are tracked by Autograd engine for backpropagation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">---&gt;</span> <span class="p">[</span> <span class="n">layer1</span> <span class="p">]</span> <span class="o">---&gt;</span> <span class="p">[</span> <span class="n">layer2</span> <span class="p">]</span> <span class="o">---&gt;</span> 
                   <span class="n">h1</span>              <span class="n">h2</span>  
</code></pre></div></div>

<p>The trick is to detach it from the <code class="highlighter-rouge">computation graph</code> so they do not consume memory.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">h2</span> <span class="o">=</span> <span class="n">layer2</span><span class="p">(</span><span class="n">layer1</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
	<span class="k">return</span> <span class="n">h2</span>
</code></pre></div></div>

<p>Encapsulating this into a gradient checkpointing block which produces the output but doesn’t save any intermediate states</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">x</span> <span class="o">---&gt;</span> <span class="p">[</span>    <span class="n">gradient</span> <span class="n">ckpt</span>   <span class="p">]</span> <span class="o">---&gt;</span> 
                               <span class="n">h2</span>  
</code></pre></div></div>

<h3 id="backward-pass">
<a class="anchor" href="#backward-pass" aria-hidden="true"><span class="octicon octicon-link"></span></a>Backward pass</h3>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># NORMAL
</span><span class="n">x</span> <span class="o">&lt;---</span> <span class="p">[</span> <span class="n">layer1</span> <span class="p">]</span> <span class="o">&lt;---</span> <span class="p">[</span> <span class="n">layer2</span> <span class="p">]</span> <span class="o">&lt;---</span>
   <span class="n">dx</span>              <span class="n">dh1</span>             <span class="n">dh2</span>

<span class="c1"># GRAD CKPT
</span><span class="n">x</span> <span class="o">&lt;---</span> <span class="p">[</span>     <span class="n">gradient</span> <span class="n">ckpt</span>      <span class="p">]</span> <span class="o">&lt;---</span>
   <span class="n">dx</span>                              <span class="n">dh2</span>
</code></pre></div></div>

<p>During the backward pass, the gradient checkpointing block needs to return <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mrow><mi>d</mi><mi>L</mi></mrow><mrow><mi>d</mi><mi>x</mi></mrow></mfrac></mrow><annotation encoding="application/x-tex">\frac{dL}{dx}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2251079999999999em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">x</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight">d</span><span class="mord mathdefault mtight">L</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>.</p>

<p>Since it’s detached from the computation graph, it needs to recompute intermediate states to produce gradient for input x. The trick is to redo the forward pass with grad-enabled then compute the gradient of activations with respect to input x</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">detach_x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
    <span class="n">h2</span> <span class="o">=</span> <span class="n">layer2</span><span class="p">(</span><span class="n">layer1</span><span class="p">(</span><span class="n">detach_x</span><span class="p">))</span>
<span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">h2</span><span class="p">,</span> <span class="n">dh2</span><span class="p">)</span>
<span class="k">return</span> <span class="n">detach_x</span><span class="o">.</span><span class="n">grad</span>
</code></pre></div></div>

<h3 id="putting-it-together">
<a class="anchor" href="#putting-it-together" aria-hidden="true"><span class="octicon octicon-link"></span></a>Putting it together</h3>

<p>Using what we learnt so far, we can create our own version of gradient checkpointing.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">detach_variable</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
                <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
                <span class="k">continue</span>
            <span class="n">x</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="n">x</span><span class="o">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="n">inp</span><span class="o">.</span><span class="n">requires_grad</span>
            <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">CkptFunc</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Function</span><span class="p">):</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="n">func</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">func</span> <span class="o">=</span> <span class="n">func</span>
        <span class="n">ctx</span><span class="o">.</span><span class="n">save_for_backward</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">outputs</span>

    <span class="o">@</span><span class="nb">staticmethod</span>
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="n">ctx</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">):</span>
        <span class="n">inputs</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">saved_tensors</span>
        <span class="n">detached_inputs</span> <span class="o">=</span> <span class="n">detach_variable</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="n">ctx</span><span class="o">.</span><span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">detached_inputs</span><span class="p">)</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="n">outputs</span> <span class="o">=</span> <span class="p">(</span><span class="n">outputs</span><span class="p">,)</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">args</span><span class="p">)</span>
        <span class="n">grads</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">inp</span><span class="o">.</span><span class="n">grad</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">inp</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">inp</span>
                      <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">detached_inputs</span><span class="p">)</span>
        <span class="k">return</span> <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="p">)</span> <span class="o">+</span> <span class="n">grads</span>
</code></pre></div></div>

<p>Let’s see how much memory it can save us. We’ll create a 40 layers neural networks with hidden state of 1024 neurons</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">clones</span><span class="p">(</span><span class="n">module</span><span class="p">,</span> <span class="n">N</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="p">[</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">module</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">)])</span>

<span class="k">class</span> <span class="nc">SubLayer</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_sz</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lin</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_sz</span><span class="p">,</span> <span class="n">hidden_sz</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Tanh</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">out</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">DummyModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_sz</span><span class="p">,</span> <span class="n">hidden_sz</span><span class="p">,</span> <span class="n">N</span><span class="p">,</span> <span class="n">use_ckpt</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lin1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_sz</span><span class="p">,</span> <span class="n">hidden_sz</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sigmoid</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layers</span> <span class="o">=</span> <span class="n">clones</span><span class="p">(</span><span class="n">SubLayer</span><span class="p">(</span><span class="n">hidden_sz</span><span class="p">),</span> <span class="n">N</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">out3</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_ckpt</span> <span class="o">=</span> <span class="n">use_ckpt</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lin1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out1</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_ckpt</span><span class="p">:</span>
        	<span class="n">x2</span> <span class="o">=</span> <span class="n">CkptFunc</span><span class="o">.</span><span class="nb">apply</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">,</span> <span class="n">x1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">x2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layers</span><span class="p">(</span><span class="n">x1</span><span class="p">)</span>
        <span class="n">x3</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out3</span><span class="p">(</span><span class="n">x2</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x3</span>
    
<span class="n">model</span> <span class="o">=</span> <span class="n">DummyModel</span><span class="p">(</span><span class="n">input_sz</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">hidden_sz</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">use_ckpt</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">N</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div></div>

<p>Result is encouraging: memory consumption with grad ckpt: <code class="highlighter-rouge">166.5352 (MB)</code> vs without <code class="highlighter-rouge">244.5352 (MB)</code>.</p>

<p>Congratulations! You just learn something really cool for your toolbox.</p>

<h3 id="references">
<a class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h3>

<p>https://github.com/pytorch/pytorch/blob/master/torch/utils/checkpoint.py</p>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="chris-tng/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/blog/pytorch/2020/08/16/gradient-checkpointing-pytorch.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/chris-tng" title="chris-tng"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
