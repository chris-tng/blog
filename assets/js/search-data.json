{
  
    
        "post0": {
            "title": "Tips on converting pytorch models to flax",
            "content": "You probably hear about jax. As all the cool kids in town use it, I decided to give it a try a while ago. But not until recently, during an experiment with TPU in Pytorch, I ran into a fatal error due to poor support of pytorch for XLA. With my limited knowledge of jax/flax, I decided to port huggingface’s ELECTRA model to flax. This results in my first huggingface pull request. This blog post contains a few tips, hopefully useful, for converting pytorch to flax model. . Why flax . You may ask why I use flax. To be honest, I don’t know. I’m a naive user just like many of you guys. Part of the reason is flax seems to be getting some traction nowadays. Especially, huggingface starts to roll out some support for flax (with FlaxBertModel). . Personally, as a pytorch user (with a painful background in tensorflow), I find flax has a steep learning curve. I don’t really like the abstractions flax introduce, it feels like magic! However, among many competing frameworks in jax space (haiku, trax, objax), I had to pick one, and that’s flax. I may deeply regret my choice in the future, who knows. But c’est la vie! . Okay, let’s start. . Loading pytorch checkpoint . First step, you may want to download the model . generator = ElectraForMaskedLM.from_pretrained(model_name) . The model binary and its JSON config are cached under ~/.cache/huggingface/transformers/ with long filenames (corresponding to Amazon S3 hashes). . You can load the binary to a python dict . import torch model_file = &quot;~/.cache/huggingface/transformers/blablabla&quot; with open(model_file, &quot;rb&quot;) as state_f: pt_state = torch.load(state_f, map_location=torch.device(&quot;cpu&quot;)) pt_state = {k: v.numpy() for k, v in pt_state.items()} . pt_state is a flat python dict, first few keys look like . &#39;electra.embeddings.word_embeddings.weight&#39;, &#39;electra.embeddings.position_embeddings.weight&#39;, &#39;electra.embeddings.token_type_embeddings.weight&#39; . jax/flax uses nested dict to manage model parameters (referred as pytrees) so a conversion is needed . Converting to nested dict . In a beautiful and happy world, we could do . from transformers import FlaxElectraForMaskedLM from flax.traverse_util import flatten_dict, unflatten_dict fx_state = FlaxElectraForMaskedLM.convert_from_pytorch(pt_state, config) fx_state = unflatten_dict({tuple(k.split(&quot;.&quot;)): v for k, v in fx_state.items()}) . Unfortunately, we don’t have FlaxElectraForMaskedLM yet, haha! So we use a similar model to load, for example FlaxBertPreTrainedModel. The key is to override convert_from_pytorch (link) so that our pytorch weights are loaded correctly in flax. . Divide and Conquer to align model weights . The whole effort lies in this part where I basically have to check that every layer is loaded correctly in flax and forward pass is done correctly. . The trick is to use scope to bind flax module. Basically, flax module works in two modes: bound and unbound. In bound mode, it keeps a reference to a scope so it has access to its parameters, and we can examine them. In unbound mode, module is no different than function, parameters are fed as arguments to the __call__ function, the module stores nothing. . from flax.core.scope import Scope from jax import random rngkey = random.PRNGKey(42) # testing on embeddings scope = Scope({&quot;params&quot;: fx_state[&quot;embeddings&quot;]}, {&quot;params&quot;: rngkey}, mutable=[&quot;params&quot;]) layer = FlaxBertEmbeddings( vocab_size=config.vocab_size, hidden_size=config.embedding_size, type_vocab_size=config.type_vocab_size, max_length=config.max_position_embeddings, parent=scope ) # checking param of layer_norm layer.children[&quot;layer_norm&quot;] # forward pass x_embed = layer(x) # check if it&#39;s close to output of pytorch model jnp.allclose(x_embed, x_embed_pt.numpy()) . If something goes wrong in this step, you will need to make changes to the above convert_to_pytorch. . Prefer setup() instead of nn.compact . One tip that helps me to debug sub-modules is to use setup instead of nn.compact. . Basically, nn.compact allows us to be lazy. It’s a decorator for the forward pass so that we can declare inlined (and lazy) sub-modules. They will be lazily initialized during the forward pass (not sure why I used so many “lazy” words, I bet it has some correlation with when I wrote this: on a Friday afternoon). . setup initializes sub-modules the moment we create the module (as of now, this hasn’t happened yet, but hopefully soon). We can examine sub-modules via attribute access like dummy.dense . from flax import linen as nn class Dummy(nn.Module): hidden_size: int def setup(self): self.dense = nn.Dense(self.hidden_size) def __call__(self, x): return self.dense(x) dummy = Dummy(hidden_size=5, parent=Scope({}, {&quot;params&quot;: rngkey}, mutable=[&quot;params&quot;])) dummy.dense . Common problems to keep in mind . flax uses kernel instead of weight for parameter: make sure to rename accordingly | sometimes, you have to transpose the weight | sometimes, you have to add missing sub-module, such as | . class FlaxElectraGeneratorPredictions(nn.Module): embedding_size: int hidden_act: str = &quot;gelu&quot; dtype: jnp.dtype = jnp.float32 def setup(self): self.dense = nn.Dense(self.embedding_size, dtype=self.dtype) self.layer_norm = FlaxElectraLayerNorm(dtype=self.dtype) def __call__(self, hidden_states): hidden_states = self.dense(hidden_states) hidden_states = ACT2FN[self.hidden_act](hidden_states) hidden_states = self.layer_norm(hidden_states) return hidden_states . Check out my code if you want to know more. . Conclusion . The process of porting a model to flax is time-consuming. I hope this post can alleviate some pains in the process. I decided to give flax a serious try. I may post something about flax or jax in the future. . I appreciate any of your feedbacks or questions, feel free to reach out. .",
            "url": "https://chris-tng.github.io/blog/transformer/nlp/flax/pytorch/jax/2020/12/16/tips-flax.html",
            "relUrl": "/transformer/nlp/flax/pytorch/jax/2020/12/16/tips-flax.html",
            "date": " • Dec 16, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Understanding linear attention in Transformers are RNNs",
            "content": "Recently, I came across an interesting paper Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Update: it was accepted in ICML 2020). In the theme of efficient transformer, this paper claims to reduce the complexity of self-attention from quadratic to linear in terms of sequence length. If that doesn’t sound exciting enough: . Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences . That sounds awesome and shiny. Let’s see what magic lies underneath. . Self-attention . A short revisit to self-attention: given 3 matrices Q, K, V corresponding to queries, keys and values. Scaled-dot product attention is computed as . V′=softmax(QKTd)VV&amp;#x27; = softmax ( frac{QK^T}{ sqrt{d}}) VV′=softmax(d . ​QKT​)V . Assume $Q$ of shape $(N, d)$, $K$ and $V$ are both of shape $(N, d)$, where $N$ is the sequence length, $d$ is model size. The attention computation becomes a bottleneck since $QK^T$ is of shape $(N, N)$. . Denote $v’_i$ is the i-th row of $V’$. Decomposing the above formula, we see that it’s computed as a linear combination of rows of $V$ : . vi′=∑jαjvj∑jαjv&amp;#x27;_i = frac{ sum_j alpha_j v_j}{ sum_j alpha_j}vi′​=∑j​αj​∑j​αj​vj​​ . αi=sim(qi,kj)=exp⁡(qi.kjTd) alpha_i = sim (q_i, k_j) = exp ( frac{q_i . k_j^T}{ sqrt{d}})αi​=sim(qi​,kj​)=exp(d . ​qi​.kjT​​) . Here is where it gets interesting. Note that instead of using exponential as the similarity function, we can use any kernel. Reminder . A kernel is a function that corresponds to a dot product in very high dimensional space. . Using this property of kernel: given a kernel K, we have $K(q_i, k_j) = phi(q_i)^T phi(k_j)$ where $ phi$ is the mapping to “high” dimensional space, called feature map. Our attention can be written as: . vi′=∑jsim(qi,kj)vj∑jsim(qi,kj)v&amp;#x27;_i = frac{ sum_j sim(q_i, k_j) v_j}{ sum_j sim(q_i, k_j)}vi′​=∑j​sim(qi​,kj​)∑j​sim(qi​,kj​)vj​​ . =∑jϕ(qi)Tϕ(kj)vj∑jϕ(qi)Tϕ(kj)= frac{ sum_j phi(q_i)^T phi(k_j) v_j}{ sum_j phi(q_i)^T phi(k_j)}=∑j​ϕ(qi​)Tϕ(kj​)∑j​ϕ(qi​)Tϕ(kj​)vj​​ . =ϕ(qi)T∑jϕ(kj)vjϕ(qi)T∑jϕ(kj)= frac{ phi(q_i)^T sum_j phi(k_j) v_j}{ phi(q_i)^T sum_j phi(k_j)}=ϕ(qi​)T∑j​ϕ(kj​)ϕ(qi​)T∑j​ϕ(kj​)vj​​ . In short . vj′=ϕ(qi)T(∑jϕ(kj)Tvj)ϕ(qi)T∑jϕ(kj)Tv&amp;#x27;_j = frac{ phi(q_i)^T ( sum_j phi(k_j)^T v_j )}{ phi(q_i)^T sum_j phi(k_j)^T}vj′​=ϕ(qi​)T∑j​ϕ(kj​)Tϕ(qi​)T(∑j​ϕ(kj​)Tvj​)​ . Basically the trick is that by kernelizing, we could compute the $KV$ product first, which results in a $(d, d)$ matrix, much cheaper than the $QK^T$ product of shape $(N, N)$. In many practical cases, we want our transformer to process long sequences, so $N$ » $d$ hence the saveup. . In terms of computation, the scaled-dot product attention would require $ mathcal{O}(d N^2)$ operations while the kernelized attention requires O(Nd2) mathcal{O}(Nd^2)O(Nd2) operations. Assume we have sequences of length 4000 with model of size 1000. Standard attention would need $16e+9$ operations, kernelized one needs $4e+9$ operations, so theoretically 4 times speedup. . Experiment . Let’s call our new attention - kernelized attention. For our experiment, we’ll use elu + 1 as our feature map . def elu(x, alpha = 0.1): mask = (x &gt; 0.).float() return mask*x + (1-mask)*alpha* (x.exp() - 1) def phi(x, alpha = 0.1): return elu(x, alpha) + 1 . Our standard scaled-dot product attention . import math def query_key_value_attn(q, k, v): &quot;q, k and v are batch-major of shape (batch_sz, seq_len, feature_sz)&quot; d_model = q.size(-1) sim = q @ k.transpose(-2, -1) / math.sqrt(d_model) return sim.softmax(-1) @ v . Kernelized attention . def kernel_attn(q, k, v, phi): assert k.size(1) == v.size(1), f&quot;Key and Value MUST have the same seq len&quot; # project onto feature space k_proj_T = phi(k.transpose(-2, -1)) q_proj = phi(q) s = k_proj_T @ v m = k_proj_T.sum(-1, keepdim=True) return (q_proj @ s) / (q_proj @ m) . On a V100 GPU, with seq_len = 4000, d_model = 1024 I see a speedup around 2.5 times (2.24ms vs 5.57ms), which is not bad. . In terms of memory, the difference is significant, when d_model is small compared to seq_len. I use d_model = 64 so the whole ting can fit into 16GB of GPU memory . Seq Len Kernelized Scaled-dot . 4096 | 12 MB | 132 MB | . 4096*6 | 72 MB | 4096 MB | . Notes . We can see kernelized attention effectively scales linearly with respect to sequence length. . It’s all nice and exciting at this point. This could be our poor man’s self-attention from now on. But keep it mind that applying masking to kernelized attention is not trivial, that could result in huge performance loss if the computation is not vectorized. Personally, I have tried to implement vectorized arbitrary masking but unsuccessful so far. Nevertheless, it’s a cool trick in our toolbox. .",
            "url": "https://chris-tng.github.io/blog/transformer/nlp/self-attention/rnn/2020/09/08/kernelized-attention.html",
            "relUrl": "/transformer/nlp/self-attention/rnn/2020/09/08/kernelized-attention.html",
            "date": " • Sep 8, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "First Look at Gradient Checkpointing in Pytorch",
            "content": "If you are one of those like me, a DL practitioner who couldn’t afford to rent a super duper 8-GPU Titan RTX or don’t have access to such compute. You might be interested in gradient checkpoint, a simple technique to trade computation for memory. In this post, I’ll explore gradient checkpointing in Pytorch. . In brief, gradient checkpointing is a trick to save memory by recomputing the intermediate activations during backward. Think of it like “lazy” backward. Layer activations are not saved for backpropagation but recomputed when necessary. To use it in pytorch: . import torch.utils.checkpoint as cp # Original: out = self.my_block(inp1, inp2, inp3) # With checkpointing: out = cp.checkpoint(self.my_block, inp1, inp2, inp3) . That looks surprisingly simple. Wondering what magic lies underneath? Let’s dive in. . Forward pass . Imagine the following forward pass: input x goes through layers one by one, results are intermediate activations h1, h2. Normally, h1 and h2 are tracked by Autograd engine for backpropagation. . x &gt; [ layer1 ] &gt; [ layer2 ] &gt; h1 h2 . The trick is to detach it from the computation graph so they do not consume memory. . with torch.no_grad(): h2 = layer2(layer1(x)) return h2 . Encapsulating this into a gradient checkpointing block which produces the output but doesn’t save any intermediate states . x &gt; [ gradient ckpt ] &gt; h2 . Backward pass . # NORMAL x &lt; [ layer1 ] &lt; [ layer2 ] &lt; dx dh1 dh2 # GRAD CKPT x &lt; [ gradient ckpt ] &lt; dx dh2 . During the backward pass, the gradient checkpointing block needs to return dLdx frac{dL}{dx}dxdL​. . Since it’s detached from the computation graph, it needs to recompute intermediate states to produce gradient for input x. The trick is to redo the forward pass with grad-enabled and compute the gradient of activations with respect to input x. . detach_x = x.detach() with torch.enable_grad(): h2 = layer2(layer1(detach_x)) torch.autograd.backward(h2, dh2) return detach_x.grad . Putting it together . Using what we learnt so far, we can create our own version of gradient checkpointing. . def detach_variable(inputs): if isinstance(inputs, tuple): out = [] for inp in inputs: if not isinstance(inp, torch.Tensor): out.append(inp) continue x = inp.detach() x.requires_grad = inp.requires_grad out.append(x) return tuple(out) class CkptFunc(torch.autograd.Function): @staticmethod def forward(ctx, func, *args): ctx.func = func ctx.save_for_backward(*args) with torch.no_grad(): outputs = func(*args) return outputs @staticmethod def backward(ctx, *args): inputs = ctx.saved_tensors detached_inputs = detach_variable(inputs) with torch.enable_grad(): outputs = ctx.func(*detached_inputs) if isinstance(outputs, torch.Tensor): outputs = (outputs,) torch.autograd.backward(outputs, args) grads = tuple(inp.grad if isinstance(inp, torch.Tensor) else inp for inp in detached_inputs) return (None, ) + grads . Let’s see how much memory it saves. . We’ll create a 40 layers neural networks with hidden state of 1024 neurons . def clones(module, N): return nn.Sequential(*[copy.deepcopy(module) for i in range(N)]) class SubLayer(nn.Module): def __init__(self, hidden_sz): super().__init__() self.lin = nn.Linear(hidden_sz, hidden_sz) self.out = nn.Tanh() def forward(self, x): x = self.lin(x) return self.out(x) class DummyModel(nn.Module): def __init__(self, input_sz, hidden_sz, N, use_ckpt=False): super().__init__() self.lin1 = nn.Linear(input_sz, hidden_sz) self.out1 = nn.Sigmoid() self.layers = clones(SubLayer(hidden_sz), N) self.out3 = nn.Softmax(dim=-1) self.use_ckpt = use_ckpt def forward(self, x): x1 = self.lin1(x) x1 = self.out1(x1) if self.use_ckpt: x2 = CkptFunc.apply(self.layers, x1) else: x2 = self.layers(x1) x3 = self.out3(x2) return x3 model = DummyModel(input_sz=64, hidden_sz=1024, use_ckpt=True, N=40) x = torch.randn(512, 64) y = model(x) . Result is encouraging: memory consumption with grad ckpt: 166.5352 (MB) vs without 244.5352 (MB). This is 30% saving in memory. . That’s it. Congratulations! You just learn something really cool for your toolbox. . References . Pytorch code for gradient ckpt | .",
            "url": "https://chris-tng.github.io/blog/pytorch/2020/08/16/gradient-checkpointing-pytorch.html",
            "relUrl": "/pytorch/2020/08/16/gradient-checkpointing-pytorch.html",
            "date": " • Aug 16, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Exploration of recent advancements in Semi-supervised Learning for NLP (part1)",
            "content": "As practitioners, I bet many would share the chronic pain of lack of data. And yet, we want to show off our shiny deep neural networks that achieves 95% accuracy. But where to get the data? Here I show you a pain reliever. It’s called semi-supervised learning. It may not be the cure since you still need some labeled data to start off, but it’s there to help. This post is my attempt to explore recent progress in semi-supervised learning with a focus in NLP. . A first-order intuition about semi-supervised learning is that it’s a regularization method in disguise. You may ask why we need regularization. It’s because training deep learning models on a small amount of labeled data is prone to overfitting. In such regime, it’s important to have strong regularization. That’s why semi-supervised learning uses unlabeled data to regularize training. . In this post, I will go over two regularization methods: consistency training and mixup. . Consistency Training says that a robust learning system should produce the same output given small perturbations of input. This means adding a small amount of noise and forcing the system to be noise-invariant actually helps training. Researches has shown that the best kind of noises is data augmentation. . Mixup says that neural network should behave linearly in-between training samples. . Experiment on YELP dataset . Vanilla Consistency training | Manifold Mixup . | UDA + Pseudo-labeling . | MixMatch | .",
            "url": "https://chris-tng.github.io/blog/semi-supervised/consistency-training/nlp/2020/08/08/semi-supervised-nlp.html",
            "relUrl": "/semi-supervised/consistency-training/nlp/2020/08/08/semi-supervised-nlp.html",
            "date": " • Aug 8, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Paper Summary: A Simple Framework for Contrastive Learning of Visual Representations",
            "content": "Context . Unsupervised learning is a framework for learning patterns in unlabeled data. Traditionally, when we think about unsupervised learning, we think about clustering like K-means, DBSCAN, etc. Until recently, with the advancements of Deep Learning, there has been a renaissance of unsupervised learning: from autoencoder to autoregressive networks, variational autoencoder and generative adversarial networks. Even now it is rebranded as self-supervised learning. . I Now call it “self-supervised learning”, because “unsupervised” is both a loaded and confusing term. In self-supervised learning, the system learns to predict part of its input from other parts of it input - Yann Lecun . Self-supevised Learning . Self-supervised learning employs clever tricks so that the data could create its own supervision. It exploits the signal in a large amount of unlabeled data to learn rich representations. . The key ingredient of self-supervised learning is the pretext task or auxilary task. Generally these tasks have some forms of reconstruction of input. In NLP, we have seen it in . Word2vec: learning to predict surrounding words given context words | BERT: learning to predict masked words given context words (Devlin et al 2018) | . In Computer Vision: . Context encoders: predict the missing part of images (Pathak et al 2016) | . SimCLR . SimCLR is the work of Google Brain that follows the line of research of applying contrastive framework into learning representation in Computer Vision (MoCo, PIRL). . The general idea is to use data augmentation to create different versions of the same example then to apply consistency training to ensure these augmented samples share the same label. The consistency training assumption is that if our data augmentation is of high quality (class-preserving data augmentation) then the augmented versions should share the same underlying labels. This assumption is reasonable and common in recent work in semi-supervised learning which employs high-quality augmentations to regularize training (FixMatch, UDA). . . SimCLR consists of the following steps: . Data Augmentation: Apply high-quality data augmentation to input x to produce a pair of augmented samples | Encoding: Run the examples through an encoding function (ResNet-50) to have a transformation-rich representation h | Projection: Project h to transformation-invariant representation z | Contrastive Learning: Representations going through a similarity measure (cosine) then propagated to a loss, which is minimized so that similar pairs are pulled together while dissimilar pairs are pushed away. | Once the model is trained on the contrastive learning task, it can be used for transfer learning. The representations from the encoder h are used instead of representations obtained from the projection head. . Result . . This is where it gets exciting . On ImageNet: A linear classifier trained on self-supervised representations matches Supervised ResNet50 (76.5% top-1, a 7% relative improvement over previous SOTA) . Basically it says that self-supervised learning models could match performance of their supervised counterparts (Of course, by using larger models and more data). Since we have way more unlabeled data than labeled ones, self-supervised learning could potentially surpass supervised learning in the near future. . Key ideas: . Composition of data augmentation is critical for learning good representations . | Projection head separated from the encoding function: it enables the model to learn transformation-rich representation. This nonlinear transformation boosts accuracy by at least 10%. . | Contrastive learning framework employs normalized temperature-scaled cross entropy loss. This loss has similar structure to npair loss which has been shown to be a powerful loss in contrastive learning. In the paper, experiments indicate that it boosts performance by +10% accuracy compared to simple margin loss. . | Overall, SimCLR is a good paper. In addition to a strong empirical result, it’s interesting to see ideas from different subfields of ML incorporated in this paper, from self-supervised to contrastive learning and semi-supervised learning. .",
            "url": "https://chris-tng.github.io/blog/self-supervised/2020/04/06/paper-summary-simclr.html",
            "relUrl": "/self-supervised/2020/04/06/paper-summary-simclr.html",
            "date": " • Apr 6, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "I’m a ML engineer/ Data scientist in the beautiful Vancouver, Canada. My technical interest is the application of Deep Learning to NLP, especially efficient transformer models, semi-supervised learning and contrastive/deep metric learning. . My passions are software engineering, machine learning and … dogs. Here is a picture of my dog to brighten your day :dog: . .",
          "url": "https://chris-tng.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
      ,"page3": {
          "title": "Projects",
          "content": "Semi-supervised NLP . Implementation of semi-supervised learning techniques: UDA, MixMatch, Mean-teacher, focusing on NLP. | . Marge . [WIP] Tentative implementation of MARGE (Pre-training via Paraphrasing) in jax/flax | . SASRec . Implementation of Self-Attentive Sequential Recommendation (SASRec) in flax. | . NRMS . [WIP] Implementation of NRMS “Neural News Recommendation with Multi-Head Self-Attention” (EMNLP2019), in Pytorch | .",
          "url": "https://chris-tng.github.io/blog/projects/",
          "relUrl": "/projects/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

}